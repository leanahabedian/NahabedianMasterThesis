\section{Resolviendo el problema de control LTS SGR(1)}

En esta sección explicaremos como una solución para un problema de control SGR(1) puede ser obtenida por construcción
utilizando técnicas existentes de síntesis de controladores (basados en estados), llamados GR(1). \cite{Piterman}

La construcción de la máquina para un problema de control LTS SGR(1) esta divido en dos pasos. Primero, se crea un juego GR(1)
$G$ en representación del ambiente $E$, las asunciones $A_s$, los objetivos $O$ y el conjunto de acciones controlables
$A_C$. Como segundo paso, se elabora una solución $(\sigma,u)$ al juego GR(1) para construir una máquina $M$ (i.e un
controlador LTS) para $\mathcal{E}$. Esta solución al problema de control LTS SGR(1) $\mathcal{E}$ existe, si y solo si,
existe una solución al juego GR(1) $G$. Luego, podremos afirmar que el controlador LTS $M$ creada a partir de
$(\sigma,u)$ es una solución a $\mathcal{E}$.

\subsection{Control LTS SGR(1) a juegos GR(1)}

Convertiremos el problema de control LTS SGR(1) a un juego GR(1). Dado un problema de control LTS SGR(1) $\mathcal{E} =
\langle E,H,A_C\rangle$ construimos un juego GR(1) $G = (S_g,\Gamma^-,\Gamma^+,s_{g_0},\varphi_g)$ tal que cada estado en $S_g$
representa un estado en $E$ y una valuación de todos los flujos que aparecen en $A_s$ y en $G$.

Mas precisamente, y por la definición de control LTS SGR(1) (definición \ref{ControlLTSSGR1}) tendremos que $H =
\{(\emptyset,I),(A_s,G)\}$, $E = (S_e,A,\Delta_e,s_{e_0})$, $A_s = \bigwedge_{i=1}^n\Box\ \Diamond\ \phi_i$, $I = \Box\
\rho$ y $G = \bigwedge_{j=1}^m\Box\ \Diamond\ \omega_j$. Sea $fl = \{\dot1,...,\dot k\}$ un conjunto de flujos usados en
$A_s$ y en $G$ donde $\dot i = \langle I_i,T_i,Init_i\rangle$. Construimos al juego $G = (S_g,\Gamma^-,\Gamma^+,s_{g_0},\varphi_g)$ de
la siguiente manera.

Construimos $S_g$ a partir de $E$ de tal forma, que los estados en $S_g$ corresponden a un estado en $E$ y los valores
de verdad de los \emph{flujos} en $\varphi$. Formalmente, tenemos que  $S_g = S_e \times \prod_{i=1}^k\{true,false\}$.
Consideramos un estado $s_g = (s_e,\alpha_1,...,\alpha_k)$. Dado un flujo $fl_i$, diremos que $s_g$ satisface $fl_i$ si
$\alpha_i$ es $true$ y $s_g$ no satisface $fl_i$ si no. 

Además, definiremos las relaciones $\Gamma^-$ y $\Gamma^+$ aplicando las siguientes reglas. Sea $s_g =
(s_e,\alpha_1,...,\alpha_k)$. Si $s_g$ no satisface $\rho$ (es decir, $s_g$ es no seguro) no agregaremos los sucesores a
$s_g$. Si $s_g$ satisface $\rho$, por cada transición $(s_e,l,s_e') \in \Delta_e$ agregaremos
$(s_g,(s_e',\alpha_1',...,\alpha_k'))$ en $\Gamma^{\beta}$, donde $\beta$ y $\alpha_i'$ cumplen las siguiente
condiciones:

\begin{center}
\begin{tabular}{ c | c}
$\beta$ & $\alpha_i'$ \\
\hline
es +: si $l \in A_C$, & es $\alpha_i$: si $l \notin I_{fl_i} \cup T_{fl_i}$, \\
es -: si $l \notin A_C$. & es $true$: si $l \in I_{fl_i}$ o \\
& es $false$: si $l \in T_{fl_i}$. \\
\end{tabular}
\end{center}

El estado inicial $s_{g_0}$ es $(s_{e_0},initially_1,...,initially_k)$.

Por último, construiremos la \emph{condición de ganada} $\varphi_g$, definida como un conjunto infinito de trazas, para
$A_S$ y $G$ de la siguiente manera: abusando de la notación denotaremos $\phi_i$ al conjunto de estados $s_g$ tales que
$s_g$ satisface las asunciones $\phi_i$ y a $\gamma_i$ al conjunto de secuencias que satisfacen
$gr((\phi_1,..,\phi_n),(\gamma_1,...,\gamma_m))$. De esta forma, obtendremos que $G = (S_g,\Gamma^-,\Gamma^+,s_{g_0},\varphi_g)$ es un
juego GR(1).

Cabe destacar que las propiedades de seguridad (safety) que son parte de la especificación no están contempladas en la
\emph{condición de ganada} $\varphi_g$ del juego GR(1), pero si se traducen a un problema de \emph{deadlock avoidance} a
la hora de construir $\Gamma^-$ y $\Gamma^+$. De esta manera, la \emph{condicion de ganada} es  $\Box\ \rho
\wedge(\bigwedge_{i=1}^n\Box\Diamond\phi_i \Rightarrow \bigwedge_{j=1}^m \Box\Diamond\omega_j)$.

\subsection{Traduciendo la estrategia a un Controlador LTS}

Ahora pasaremos a explicar como conseguir un controlador LTS a partir de una estrategia ganadora para el juego en GR(1).
Intuitivamente, la transformación es de la siguiente manera: dado un problema de control LTS SGR(1) $\mathcal{E} =
\langle E,H,A_C\rangle$, el juego $G = (S_g, \Gamma^-, \Gamma^+, s_{g_0},\varphi_g)$ obtenido a partir de $\mathcal{E}$ y de la
estrategia ganadora para $G$, construimos $M = (S_M,A,\Delta_M,s_{M_0})$ una solución para $\mathcal{E}$ traduciendo
a estados de $S_M$ un estado de $S_g$ y un estado de la memoria dada por la estrategia ganadora.

Mas formalmente, sea $E = (S_e,A,\Delta_e,s_{e_0})$, $fl = \{fl_1,...,fl_k\}$ el conjunto de flujos que aparecen en
$\varphi$, $G = (S_g,\Gamma^-,\Gamma^+,s_{g_0},\varphi_g)$ el juego GR(1) construido a partir de $E$ como explicamos
anteriormente, y sea $\sigma: \Omega \times S_g \rightarrow 2^{S_g}$ y $u: \Omega \times S_g \rightarrow \Omega$ la
estrategia ganadora para $G$. Construiremos la máquina $M = (S_M, A, \Delta_M, s_{M_0})$ de la siguiente manera.

Para construir $S_M \subseteq \Omega \times S_g$, consideremos dos estados $s_g = (s_e,\alpha_1,...,\alpha_k)$ y $s_g' =
(s_e',\alpha_1',...,\alpha_k')$. Decimos que esa acción $\ell$ es \emph{posible} desde $s_g$ hacia $s_g'$ si:

\begin{enumerate}
\itemsep-4mm
\item $(s_g,s_g') \in \Gamma^- \cup \Gamma^+$,
\item existe una acción $\ell$ tal que $(s_e,\ell,s_e') \in \Delta_e$ y
\item para cada \emph{flujo} $fl_i$ valga alguna de las siguiente condiciones:
\vspace{-4mm}
\begin{itemize}
    \itemsep-4mm
    \item $\ell \notin I_{fl_i} \cup T_{fl_i}$ y $\alpha_i' = \alpha_i$,
    \item $\ell \in I_{fl_i}$ y $\alpha_i'=true$, o
    \item $\ell \in T_{fl_i}$ u $\alpha_i'=false$.
\end{itemize}
\end{enumerate}

Para construir $\Delta_M \subset S_M \times A \times S_M$, consideremos la transición $(s_g,s_g') \in \Gamma^-$. Por
definición de $\Gamma^-$ existe una acción $\ell \notin A_C$ tal que $\ell$ es posible desde $s_g$ hacia $s_g'$. Si
$s_g' \in \sigma(\omega,s_g)$ entonces para cada acción $\ell$ tal que $\ell$ es posible desde $s_g$ hacia $s_g'$
agregamos $((\omega,s_g),\ell,(u(\omega,s_g),s_g'))$ hacia $\Delta_M$. De forma similar, consideramos una transición
$(s_g,s_g') \in \Gamma^+$. Por definición de $\Gamma^+$ existe una acción $\ell \in A_C$ tal que $\ell$ es posible desde
$s_g$ hacia $s_g'$. Si $s_g' \in \sigma(\omega,s_g)$ entonces para cada acción $\ell$ tal que $\ell$ es posible desde
$s_g$ hacia $s_g'$ agregamos $((\omega,s_g),\ell,(u(\omega,s_g),s_g'))$ hacia $\Delta_M$.

El estado inicial de $M$ esta definido como $s_{M_0} = (\omega_0,s_{g_0})$ donde $\omega_0$ es el valor inicial de la
memoria $\Omega$. De esta forma completamos la definición de $M$.

\subsection{Algoritmo}

En esta sección, presentaremos el algoritmo implementado en la herramienta MTSA \cite{4639371} el cual está basado en las
ideas de Juvekar y Piterman \cite{Juvekar:buchi}.

Este algoritmo realiza una búsqueda de ciclos de estados que satisfacen todas las asunciones pero no todos los objetivos
restringiendo acciones controlables. De haber ciclos como estos podrían permitir tranzas en las que el controlador
pierde el juego GR(1). Para lograr evitar estos ciclos, el algoritmo busca para cada estado, una estrategia que
garantice la satisfacción de todos los objetivos. Para esto, se configura un orden en el cual satisfacer los objetivos.
El algoritmo, mediante la técnica de punto fijo computa la mejor forma en que cada estado puede satisfacer el siguiente
objetivo. A su vez, mide la ''calidad'' de cada uno de los diferentes sucesores para satisfacer un objetivo mediante un
sistema de rankings \cite{Jurdzinski:ParityGames}. El ranking de un sucesor particular mide la distancia (cantidad de transiciones utilizadas)
al siguiente objetivo en términos de número de veces que las asunciones son satisfechas antes de alcanzar el objetivo.
Si este número tiende a infinito, deduciremos que desde el estado actual existe una traza infinita en la cual las
asunciones del ambiente valen infinitamente, pero los objetivos no se satisfacen. Es así, como el algoritmo reconoce
estados que deben ser evitados para la construcción de la estrategia para el controlador.

\begin{nahaDef}
    \emph{(Función de Ranking) Sea $G = (S_g,\Gamma^-,\Gamma^+,s_{g_0},\varphi)$ donde \\$\varphi = 
    gr(( \phi_1 ,..., \phi_n ),( \gamma_1 ,..., \gamma_m ) )$. Una función de ranking para un objetivo $\gamma_j$ es una función
    $R_j : S_g \rightarrow (\mathbb{N} \times \{1,...,n\}\cup\{\infty\})$. Intuitivamente, $R_j(s_g) = (k,\ell)$
    significa que para alcanzar desde $s_g$ a un estado en el cual $\gamma_j$ vale, todos los caminos satisfacen
    la asunción $\phi_{\ell}$ a lo sumo $k$ veces. $R(s) = \infty$ significa que $s$ es un estado perdedor, es decir,
    desde $s$ no hay una estrategia para el controlador que pueda evitar una traza en la cual se satisface infinitamente
    las asunciones, pero no satisface infinitamente a todos los objetivos.}
\end{nahaDef}

\begin{algorithm}
\caption{para resolver juegos SGR(1)}\label{algoritmo}
\begin{algorithmic}[1]
\Procedure{solveGame(game=(states,transitions),safe,guarantees,assumptions)}{}
\State {// \emph{Inicialización}:}
\For {state : states}
    \For {g : guarantees}
        \State $rank_g$(state)$\gets$ (0,1)
    \EndFor
\EndFor
\State {// Queue pending}
\For {state : states}
    \If {$\exists$ g : guarantees / state $\notin$ g $\land$ state $\in assume_1$}
        \State pending.push(pair(state,g))
    \EndIf
    \If {$\Gamma^-$(state) = $\emptyset \land \Gamma^+(state)$ = $\emptyset$}
        \For {g : guarantees}
            \State $rank_g$(state) $\gets \infty$
            \State pending.push(unstable\_pred(state,g))
        \EndFor
    \EndIf
\EndFor
\State {// \emph{Estabilización}:}
\While {!pending.empty()}
    \State (state,g) $\gets$ pending.pop()
    \If {$rank_g$(state) = $\infty$ }
        \State continue
    \EndIf
    \If {isStable($rank_g$(state))}
        \State continue
    \EndIf
    \State $rank_g$(state) $\gets$ inc(best(state,g),state,g)
    \State pending.push(unstable\_pred(state,g))
\EndWhile
\EndProcedure
\end{algorithmic}
\end{algorithm}


El algoritmo \ref{algoritmo} computa un ranking estable en cada estado $s_g \in T$ si $s_g$ es ganador para el
controlador (es decir, $R_1(t)<\infty$). Conceptualmente, podemos separar el algoritmo en dos grandes instancias,
inicialización y estabilización. El valor inicial del ranking para cada estado en el juego, junto a la cola de
estados \emph{pending} para ser procesados, se crean en la etapa de inicialización. Agregaremos un estado a
\emph{pending} si no satisface ningún objetivo y satisface las asunciones. Todos los estados en cada función de ranking
son inicializados con el valor $(0,1)$. Este valor indica el menor ranking posible. Los estados que cumplen que
$\Gamma^- \cup \Gamma^+ = \emptyset$ serán inicializados con el valor $\infty$. De esta manera, los estados cuyos
rankings son $\infty$ son aquellos donde no se satisface $\rho$ o son estados de \emph{deadlock} en $E$.

La sección de estabilización es un iteración de punto fijo sobre la cola \emph{pending} hasta que se vacía. La función
\texttt{is\_stable(state,g)} devuelve true si la g-esima función de ranking es estable para \texttt{state}.

La función \texttt{unstable\_pred(state,g)} devuelve un conjunto de pares de predecesores de \texttt{state} y un
ranking g para el cual el ranking es inestable.

La función \texttt{best(state,g)} devuelve el mejor ranking basado en sus sucesores. Para eso utiliza la siguiente
función $sr : S_g \rightarrow (\mathbb{N} \times \{1,...,n\} \cup \{\infty\})$. Esta función también codifica el hecho
de que los estados de \emph{deadlock} tienen ranking $\infty$. Además, notemos que usa un orden lexicográfico para los
objetivos. Dado un estado $s_g$ y un objetivo $\gamma_j$, $sr(s_g,j)$ está definida de la siguiente manera:

\begin{itemize}
\itemsep-4mm
\item Si $\Gamma^+(s_g) \cup \Gamma^-(s_g) = \emptyset$, entonces $sr(s_g,j) = \infty$, caso contrario,
\item si $s_g$ es controlable y $s_g \in \gamma_j$, entonces $sr(s_g,j) = min_{s'_g \in \Gamma^+(s_g)} R_{j \oplus
1}(S'_g)$.
\item si $s_g$ es controlable y $s_g \notin \gamma_j$, entonces $sr(s_g,j) = min_{s'_g \in \Gamma^+(s_g)} R_j(s'_g)$.
\item si $s_g$ es no controlable y $s_g \in \gamma_j$, entonces $sr(s_g,j) = max_{s'_g \in \Gamma^-(s_g)} R_{j \oplus
1}(s'_g)$.
\item si $s_g$ es no controlable y $s_g \notin \gamma_j$, entonces $sr(s_g,j) = max_{s'_g \in \Gamma^-(s_g)} R_j(s'_g)$.
\end{itemize}

Por último, \texttt{inc(($k$,$\ell$),state,g)} devuelve $(0,1)$ si \texttt{state} está en $\gamma_g$, devuelve $(k,\ell)$ si
\texttt{state} no está en \texttt{$assumption_\ell$}, y devuelve el mínimo valor mayor que $(k,\ell)$ en el resto de los
casos. Notemos que \texttt{inc($\infty$,state,g)} es $\infty$, y si $n = max_\ell(|\phi_\ell-(\gamma_g)|)$ y
\texttt{state} está en $\phi_m-\gamma_g$ entonces \texttt{inc((n,m),state,g)} es $\infty$. Este algoritmo calcula el
mínimo ranking estable. Basados en ideas del mundo de autómatas de büchi \cite{doi:10.1137/S0097539703420675,
PitermanAndJuvekar}, este algoritmo puede ser implementado
en $O(m \cdot n \cdot |S|^2)$.
